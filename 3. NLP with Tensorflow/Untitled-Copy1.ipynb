{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b503540-8003-4ca2-b985-990dbdf84195",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae869b8-e095-47aa-8421-89454b901161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4f8fdf-9020-4f5e-bfe9-0f9544f55819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "data = dd.read_json(\"/home/sharoonsaxena/Datasets/Sarcasm.json\").repartition(8)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ac402f-943f-4b79-9f8b-d7d9bd26d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train validation partition\n",
    "train, validation = data.random_split([0.8, 0.2], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eec4a54-c6f7-4e31-993d-4f3e974d8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21317,), numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolating headlines and labels\n",
    "train_headline = train.headline.values.compute()\n",
    "train_labels = train.is_sarcastic.values.compute()\n",
    "\n",
    "val_headline = validation.headline.values.compute()\n",
    "val_labels = validation.is_sarcastic.values.compute()\n",
    "\n",
    "train_headline.shape, type(train_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07918beb-7d4a-458a-b936-7fd215cc9963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26491"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting hyper-params\n",
    "W_MAX_LEN = 200\n",
    "W_VOCAB = 20000\n",
    "EMBED_DIM = 32\n",
    "PADDING = \"post\"\n",
    "TRUNC = \"POST\"\n",
    "OOV = \"<OOV>\"\n",
    "\n",
    "# word tokenizer\n",
    "word_tokenizer = Tokenizer(num_words=W_MAX_LEN, oov_token=OOV)\n",
    "word_tokenizer.fit_on_texts(train_headline)\n",
    "len(word_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff18fcf3-26fe-443a-bdad-4340838ddb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting hyper-params\n",
    "C_MAX_LEN = 500\n",
    "C_VOCAB = 100\n",
    "EMBED_DIM = 32\n",
    "PADDING = \"post\"\n",
    "TRUNC = \"post\"\n",
    "OOV = \"<OOV>\"\n",
    "\n",
    "# word tokenizer\n",
    "char_tokenizer = Tokenizer(\n",
    "    num_words=C_MAX_LEN,\n",
    "    oov_token=OOV,\n",
    "    char_level=True,\n",
    ")\n",
    "char_tokenizer.fit_on_texts(train_headline)\n",
    "len(char_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bd4e4e-b2ea-4a16-9719-8b513f29f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21317, list)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texts to sequences using word tokenizer\n",
    "train_word_sequences = word_tokenizer.texts_to_sequences(train_headline)\n",
    "val_word_sequences = word_tokenizer.texts_to_sequences(val_headline)\n",
    "\n",
    "len(train_word_sequences), type(train_word_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9909d67-7e15-492f-aeaa-5ca41114657f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21317, list)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texts to sequences using char tokenizer\n",
    "train_char_sequences = char_tokenizer.texts_to_sequences(train_headline)\n",
    "val_char_sequences = char_tokenizer.texts_to_sequences(val_headline)\n",
    "\n",
    "len(train_char_sequences), type(train_char_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98cb8eb4-74a8-422e-b15a-af5e692c4287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21317, numpy.ndarray)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding word sequences\n",
    "train_word_padded_sequences = pad_sequences(\n",
    "    train_word_sequences, maxlen=W_MAX_LEN, padding=PADDING, truncating=TRUNC\n",
    ")\n",
    "val_word_padded_sequences = pad_sequences(\n",
    "    val_word_sequences, maxlen=W_MAX_LEN, padding=PADDING, truncating=TRUNC\n",
    ")\n",
    "\n",
    "len(train_word_padded_sequences), type(train_word_padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e6c467-2d27-45f7-8e9c-626f23208658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21317, numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding char sequences\n",
    "train_char_padded_sequences = pad_sequences(\n",
    "    train_char_sequences, maxlen=W_MAX_LEN, padding=PADDING, truncating=TRUNC\n",
    ")\n",
    "val_char_padded_sequences = pad_sequences(\n",
    "    val_char_sequences, maxlen=W_MAX_LEN, padding=PADDING, truncating=TRUNC\n",
    ")\n",
    "\n",
    "len(train_char_padded_sequences), type(train_char_padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "654b781d-8815-457c-9072-88e54bef2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the <OOV> <OOV> <OOV> to see your <OOV> in the <OOV>                                                                                                                                                                                             \n",
      "the science-backed reason to see your therapist in the morning\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_tokenizer.word_index.items()]\n",
    ")\n",
    "\n",
    "\n",
    "def decode_word_sentence(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"\") for i in text])\n",
    "\n",
    "\n",
    "print(decode_word_sentence(train_word_padded_sequences[1]))\n",
    "print(train_headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6eafad-7989-46f6-a133-9db81315afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the science-backed reason to see your therapist in the morning\n",
      "the science-backed reason to see your therapist in the morning\n"
     ]
    }
   ],
   "source": [
    "reverse_char_index = dict(\n",
    "    [(value, key) for (key, value) in char_tokenizer.word_index.items()]\n",
    ")\n",
    "\n",
    "\n",
    "def decode_char_sentence(text):\n",
    "    return \"\".join([reverse_char_index.get(i, \"\") for i in text])\n",
    "\n",
    "\n",
    "print(decode_char_sentence(train_char_padded_sequences[1]))\n",
    "print(train_headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0895d304-17f6-4dde-88bd-371881496b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# converting everything to list\n",
    "train_word_padded_sequences.tolist(), train_labels.tolist()\n",
    "val_word_padded_sequences.tolist(), val_labels.tolist()\n",
    "\n",
    "train_char_padded_sequences.tolist()\n",
    "val_char_padded_sequences.tolist()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192db1a-fb1d-4306-9807-fb3a1b76e95f",
   "metadata": {},
   "source": [
    "### Model 1: embedding model at word and character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4ccd0d-4bd0-40a4-9f12-9f7de3f23f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           640000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                102416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 742,433\n",
      "Trainable params: 742,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_embed_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=W_VOCAB,\n",
    "            output_dim=EMBED_DIM,\n",
    "            input_length=W_MAX_LEN,\n",
    "        ),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=16, activation=\"relu\"),\n",
    "        layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "word_embed_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "word_embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2d5488-d21c-49c7-8b8a-9159cb74cf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 0.5094 - accuracy: 0.7289 - val_loss: 0.4349 - val_accuracy: 0.7787\n",
      "Epoch 2/5\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 0.4210 - accuracy: 0.7957 - val_loss: 0.4121 - val_accuracy: 0.8016\n",
      "Epoch 3/5\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 0.4094 - accuracy: 0.8006 - val_loss: 0.4128 - val_accuracy: 0.7991\n",
      "Epoch 4/5\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 0.4022 - accuracy: 0.8041 - val_loss: 0.4158 - val_accuracy: 0.7971\n",
      "Epoch 5/5\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 0.3962 - accuracy: 0.8058 - val_loss: 0.4102 - val_accuracy: 0.7993\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "history_word_embeds = word_embed_model.fit(\n",
    "    x=train_word_padded_sequences,\n",
    "    y=train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_word_padded_sequences, val_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62ddadbe-db14-4b0e-b0ae-d2ec5e07ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           3200      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                256016    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 259,233\n",
      "Trainable params: 259,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_embed_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=C_VOCAB,\n",
    "            output_dim=EMBED_DIM,\n",
    "            input_length=C_MAX_LEN,\n",
    "        ),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=16, activation=\"relu\"),\n",
    "        layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "char_embed_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "char_embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d53a8-3bb2-43c3-956c-3c9ca9e6754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 0.6420 - accuracy: 0.6265 - val_loss: 0.5887 - val_accuracy: 0.6808\n",
      "Epoch 2/5\n",
      "104/667 [===>..........................] - ETA: 3s - loss: 0.5646 - accuracy: 0.6971"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "history_char_embeds = word_embed_model.fit(\n",
    "    x=train_char_padded_sequences,\n",
    "    y=train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_char_padded_sequences, val_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db6f85-9fc4-4314-aeb6-b96a8fdf5475",
   "metadata": {},
   "source": [
    "### Model 2: RNN at character and Word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280a1b5-4625-4055-860a-2f3418de61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_GRU_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=W_VOCAB,\n",
    "            output_dim=EMBED_DIM,\n",
    "            input_length=W_MAX_LEN,\n",
    "        ),\n",
    "        layers.Bidirectional(layers.GRU(16, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.GRU(16)),\n",
    "        layers.Dense(units=16, activation=\"relu\"),\n",
    "        layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "word_GRU_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "word_GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a51a1-ddd2-436a-bc87-73170830785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "history_word_rnn = word_rnn_model.fit(\n",
    "    x=train_word_padded_sequences,\n",
    "    y=train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_word_padded_sequences, val_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e99c89-0de9-4539-bc8e-300f259b9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=C_VOCAB,\n",
    "            output_dim=EMBED_DIM,\n",
    "            input_length=C_MAX_LEN,\n",
    "        ),\n",
    "        layers.Bidirectional(layers.RNN(units=16, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.RNN(units=16))\n",
    "        layers.Dense(units=16, activation=\"relu\"),\n",
    "        layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "char_embed_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "char_embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2decb3-0aa2-4505-8aed-9fe1f4737714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
