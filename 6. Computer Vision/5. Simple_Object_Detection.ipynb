{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmANPR2jhCR6"
      },
      "source": [
        "# Simple Object Detection in Tensorflow\n",
        "\n",
        "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
        "\n",
        "* explore the Tensorflow Hub for object detection models\n",
        "* load the models in your workspace\n",
        "* preprocess an image for inference \n",
        "* run inference on the models and inspect the output\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkMLuGDhCR6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEoRKdmByrb0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb8MBgTOhCR6"
      },
      "source": [
        "### Download the model from Tensorflow Hub\n",
        "\n",
        "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects. \n",
        "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories. \n",
        "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection). \n",
        "- You can select a model to see more information about it and copy the URL so you can download it to your workspace. \n",
        "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
        "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C9pCzz4uy20U"
      },
      "outputs": [],
      "source": [
        "# you can switch the commented lines here to pick the other model\n",
        "\n",
        "# inception resnet version 2\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "\n",
        "# You can choose ssd mobilenet version 2 instead and compare the results\n",
        "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3trj5FbhCR6"
      },
      "source": [
        "#### Load the model\n",
        "\n",
        "Next, you'll load the model specified by the `module_handle`.\n",
        "- This will take a few minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0WHkGDHfhCR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "2022-08-03 10:19:33.585817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.590190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.590510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.591172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-08-03 10:19:33.591785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.592073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.592357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.982483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.982804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.983149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-03 10:19:33.983403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2114 MB memory:  -> device: 0, name: NVIDIA T600, pci bus id: 0000:0a:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "model = hub.load(module_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ey0FpHGhCR6"
      },
      "source": [
        "#### Choose the default signature\n",
        "\n",
        "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model. \n",
        "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X1BU7AGthCR6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x7F8CFBF635B0>}))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take a look at the available signatures for this particular model\n",
        "model.signatures.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfc9ax9hhCR6"
      },
      "source": [
        "Please choose the 'default' signature for your object detector.\n",
        "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pzwR5zE_hCR7"
      },
      "outputs": [],
      "source": [
        "detector = model.signatures['default']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb-3r3thCR7"
      },
      "source": [
        "### download_and_resize_image\n",
        "\n",
        "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ucsxak_qhCR7"
      },
      "outputs": [],
      "source": [
        "def download_and_resize_image(url, new_width=256, new_height=256):\n",
        "    '''\n",
        "    Fetches an image online, resizes it and saves it locally.\n",
        "    \n",
        "    Args:\n",
        "        url (string) -- link to the image\n",
        "        new_width (int) -- size in pixels used for resizing the width of the image\n",
        "        new_height (int) -- size in pixels used for resizing the length of the image\n",
        "        \n",
        "    Returns:\n",
        "        (string) -- path to the saved image\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # create a temporary file ending with \".jpg\"\n",
        "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "    \n",
        "    # opens the given URL\n",
        "    response = urlopen(url)\n",
        "    \n",
        "    # reads the image fetched from the URL\n",
        "    image_data = response.read()\n",
        "    \n",
        "    # puts the image data in memory buffer\n",
        "    image_data = BytesIO(image_data)\n",
        "    \n",
        "    # opens the image\n",
        "    pil_image = Image.open(image_data)\n",
        "    \n",
        "    # resizes the image. will crop if aspect ratio is different.\n",
        "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "    \n",
        "    # converts to the RGB colorspace\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "    \n",
        "    # saves the image to the temporary file created earlier\n",
        "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "    \n",
        "    print(\"Image downloaded to %s.\" % filename)\n",
        "    \n",
        "    return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qodEJHhCR7"
      },
      "source": [
        "### Download and preprocess an image\n",
        "\n",
        "Now, using `download_and_resize_image` you can get a sample image online and save it locally. \n",
        "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
        "- You can use the original width and height of the image but feel free to modify it and see what results you get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xHTDalVrhCR7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded to /tmp/tmpby0s38h9.jpg.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_89903/3150208785.py:31: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
            "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "# You can choose a different URL that points to an image of your choice\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
        "\n",
        "# download the image and use the original height and width\n",
        "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNXUKMIhCR7"
      },
      "source": [
        "### run_detector\n",
        "\n",
        "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
        "- run_detector uses `load_image` to convert the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wkkiQzKlhCR7"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    '''\n",
        "    Loads a JPEG image and converts it to a tensor.\n",
        "    \n",
        "    Args:\n",
        "        path (string) -- path to a locally saved JPEG image\n",
        "    \n",
        "    Returns:\n",
        "        (tensor) -- an image tensor\n",
        "    '''\n",
        "    \n",
        "    # read the file\n",
        "    img = tf.io.read_file(path)\n",
        "    \n",
        "    # convert to a tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "def run_detector(detector, path):\n",
        "    '''\n",
        "    Runs inference on a local file using an object detection model.\n",
        "    \n",
        "    Args:\n",
        "        detector (model) -- an object detection model loaded from TF Hub\n",
        "        path (string) -- path to an image saved locally\n",
        "    '''\n",
        "    \n",
        "    # load an image tensor from a local file path\n",
        "    img = load_img(path)\n",
        "\n",
        "    # add a batch dimension in front of the tensor\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "    \n",
        "    # run inference using the model\n",
        "    result = detector(converted_img)\n",
        "\n",
        "    # save the results in a dictionary\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    # print results\n",
        "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "\n",
        "    print(result[\"detection_scores\"])\n",
        "    print(result[\"detection_class_entities\"])\n",
        "    print(result[\"detection_boxes\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEeJSkxhCR7"
      },
      "source": [
        "### Run inference on the image\n",
        "\n",
        "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists: \n",
        "\n",
        "* The detection scores of each object found (i.e. how confident the model is), \n",
        "* The classes of each object found, \n",
        "* The bounding boxes of each object\n",
        "\n",
        "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "csanHvDIz4_t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-03 10:20:19.824055: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -2642 } dim { size: -2643 } dim { size: -2644 } dim { size: 1088 } } } inputs { dtype: DT_FLOAT shape { dim { size: -22 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -22 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 17 } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA T600\" frequency: 1335 num_cores: 10 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 1048576 shared_memory_size_per_multiprocessor: 65536 memory_size: 2216820736 bandwidth: 160032000 } outputs { dtype: DT_FLOAT shape { dim { size: -22 } dim { size: 17 } dim { size: 17 } dim { size: 1088 } } }\n",
            "2022-08-03 10:20:32.130051: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
            "2022-08-03 10:20:32.523701: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
            "\n",
            "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
            "2022-08-03 10:20:33.282128: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.75GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.282184: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.75GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.304836: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.57GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.304891: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.57GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.573984: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.574027: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.701090: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:33.701153: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:36.925862: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.33GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2022-08-03 10:20:36.925905: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.33GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100 objects.\n",
            "[0.6544859  0.6114538  0.6042276  0.5926313  0.59219265 0.5804911\n",
            " 0.5514052  0.4946692  0.47515708 0.47342214 0.4399585  0.4148513\n",
            " 0.4062968  0.3982887  0.39765206 0.3762105  0.37279463 0.36574852\n",
            " 0.35260734 0.33274773 0.3042876  0.27276558 0.26865068 0.2577718\n",
            " 0.25290582 0.24612142 0.23403868 0.20342907 0.18229398 0.1804574\n",
            " 0.17571343 0.16435064 0.15850061 0.15665999 0.15470928 0.15452762\n",
            " 0.14924906 0.13340679 0.12948298 0.1264971  0.12044206 0.11767376\n",
            " 0.11356082 0.11114815 0.11100291 0.10914944 0.10604057 0.08940503\n",
            " 0.08598275 0.0828018  0.08104554 0.07806055 0.07760363 0.07628638\n",
            " 0.07546856 0.07444118 0.07427152 0.07204893 0.07177527 0.07102224\n",
            " 0.07032738 0.06809704 0.06304484 0.06285896 0.06270944 0.06223938\n",
            " 0.05882125 0.05815    0.0579581  0.05787583 0.05462381 0.05274339\n",
            " 0.05133714 0.04826542 0.0470842  0.04682819 0.04495228 0.04405158\n",
            " 0.04360709 0.04113491 0.04109957 0.03968568 0.03934942 0.03912799\n",
            " 0.03879525 0.038786   0.0373964  0.03606922 0.03367086 0.03366853\n",
            " 0.03260196 0.03253535 0.0320147  0.02983097 0.0287799  0.02867633\n",
            " 0.02803948 0.0278319  0.02734359 0.02668229]\n",
            "[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n",
            " b'Bicycle' b'Building' b'Window' b'Person' b'Bicycle' b'Wheel'\n",
            " b'Building' b'Building' b'Building' b'Person' b'Wheel' b'Window'\n",
            " b'Window' b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel'\n",
            " b'Person' b'Window' b'Window' b'Building' b'Window' b'Window' b'Man'\n",
            " b'Person' b'Woman' b'Person' b'Clothing' b'Bicycle wheel' b'Window'\n",
            " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing' b'Window'\n",
            " b'Bicycle' b'Land vehicle' b'House' b'House' b'Man' b'Window' b'Clothing'\n",
            " b'Window' b'Footwear' b'Person' b'Man' b'Man' b'House' b'Building'\n",
            " b'Person' b'Clothing' b'Window' b'Person' b'Man' b'Person' b'Furniture'\n",
            " b'Jeans' b'Person' b'Person' b'Person' b'Land vehicle' b'Window' b'House'\n",
            " b'Woman' b'Man' b'Window' b'Person' b'Person' b'Clothing' b'Man' b'Man'\n",
            " b'Window' b'Car' b'Person' b'Man' b'Chair' b'Car' b'House' b'Window'\n",
            " b'Tire' b'Clothing' b'Window' b'Clothing' b'Land vehicle' b'Window'\n",
            " b'Window' b'Man' b'Van' b'Bus' b'Clothing' b'Car']\n",
            "[[5.1279444e-01 5.2927095e-01 6.0166234e-01 5.5209446e-01]\n",
            " [5.1974601e-01 6.0150713e-01 6.4612418e-01 6.3468289e-01]\n",
            " [5.0574601e-01 5.0044078e-01 6.0134917e-01 5.2308965e-01]\n",
            " [4.8630881e-01 4.1276225e-01 6.7855030e-01 4.5990565e-01]\n",
            " [8.1519085e-01 9.5611840e-01 8.4270173e-01 9.8714471e-01]\n",
            " [4.9546641e-01 9.2353427e-01 8.3563489e-01 9.9905688e-01]\n",
            " [1.1098607e-02 1.1912091e-02 7.3975039e-01 4.2490724e-01]\n",
            " [5.7782596e-01 3.6645326e-01 7.1280575e-01 4.8333818e-01]\n",
            " [7.7493593e-02 4.1305402e-01 5.7945883e-01 5.6030923e-01]\n",
            " [0.0000000e+00 1.1929258e-01 2.2389720e-01 1.8394905e-01]\n",
            " [5.1406980e-01 7.4809784e-01 5.9196222e-01 7.6656920e-01]\n",
            " [5.7077783e-01 3.6182037e-01 7.0732844e-01 4.2966676e-01]\n",
            " [6.3209414e-01 3.5986990e-01 7.0384169e-01 4.1181558e-01]\n",
            " [1.5908586e-02 6.8496162e-01 5.5938888e-01 8.1114680e-01]\n",
            " [0.0000000e+00 7.9710943e-01 6.7373610e-01 1.0000000e+00]\n",
            " [0.0000000e+00 2.1702690e-01 6.5097314e-01 4.3200094e-01]\n",
            " [5.0037277e-01 3.7700447e-01 6.3335049e-01 4.1451439e-01]\n",
            " [6.4033997e-01 4.4502339e-01 7.0303476e-01 4.8345748e-01]\n",
            " [1.9440333e-03 0.0000000e+00 1.3933197e-01 2.6288418e-02]\n",
            " [2.5519116e-03 9.6662557e-01 1.5375262e-01 1.0000000e+00]\n",
            " [1.4155071e-03 1.4104929e-03 7.6484835e-01 2.6935190e-01]\n",
            " [5.0490111e-01 3.6078489e-01 6.3766336e-01 3.8548014e-01]\n",
            " [4.8338380e-01 6.1948413e-01 5.6265801e-01 6.6157210e-01]\n",
            " [4.9820140e-01 3.6461407e-01 6.6115755e-01 4.0489638e-01]\n",
            " [6.3122934e-01 3.6032289e-01 7.0414704e-01 4.1149938e-01]\n",
            " [5.2180678e-01 5.7769489e-01 5.8761311e-01 6.0071784e-01]\n",
            " [2.1960370e-01 3.4873888e-01 3.3825555e-01 3.7706766e-01]\n",
            " [1.2482677e-01 2.5092393e-01 2.7991477e-01 2.8162587e-01]\n",
            " [2.5731847e-01 5.6749362e-01 5.3090996e-01 6.8787652e-01]\n",
            " [4.2175390e-02 8.7476528e-01 2.5286335e-01 9.1304618e-01]\n",
            " [1.5640162e-01 4.4336551e-01 2.2223382e-01 4.7578454e-01]\n",
            " [5.0199443e-01 9.2146742e-01 8.3636171e-01 1.0000000e+00]\n",
            " [5.2367359e-01 5.7034701e-01 5.8450609e-01 5.9160703e-01]\n",
            " [5.1916909e-01 5.9996605e-01 6.4633018e-01 6.3409466e-01]\n",
            " [5.1315480e-01 6.7922854e-01 5.5098128e-01 6.9254810e-01]\n",
            " [5.2434462e-01 9.2494547e-01 8.1052816e-01 9.9797946e-01]\n",
            " [6.3806325e-01 4.4279727e-01 7.0172906e-01 4.8413196e-01]\n",
            " [3.4105543e-02 3.5565761e-01 1.6230486e-01 3.7490875e-01]\n",
            " [4.8809028e-01 4.5336694e-01 6.2225717e-01 4.7966492e-01]\n",
            " [9.6650125e-04 3.0770731e-01 1.0651585e-01 3.3207035e-01]\n",
            " [4.8297009e-01 6.1979163e-01 5.6477898e-01 6.6065258e-01]\n",
            " [5.8239120e-01 3.6492342e-01 7.1389169e-01 4.8468536e-01]\n",
            " [5.2378994e-01 7.4929273e-01 5.8547032e-01 7.6531148e-01]\n",
            " [3.5146433e-01 9.7486877e-01 5.5304360e-01 9.9888706e-01]\n",
            " [6.0907686e-01 4.2683351e-01 7.0519632e-01 4.8710752e-01]\n",
            " [5.6925470e-01 3.5978302e-01 7.0856631e-01 4.2843863e-01]\n",
            " [0.0000000e+00 8.1118721e-01 6.9358277e-01 9.9325359e-01]\n",
            " [1.0429535e-02 2.2947056e-02 7.2731256e-01 4.2228749e-01]\n",
            " [4.8463222e-01 4.1069773e-01 6.9474280e-01 4.6313992e-01]\n",
            " [8.1154451e-02 3.8477594e-01 2.0795213e-01 4.1175538e-01]\n",
            " [5.3856730e-01 6.0358500e-01 6.3474083e-01 6.3447654e-01]\n",
            " [0.0000000e+00 1.2407584e-02 1.4029649e-01 2.4734123e-02]\n",
            " [6.2977993e-01 6.1488342e-01 6.4490789e-01 6.2533504e-01]\n",
            " [5.0284290e-01 3.8242072e-01 5.9601647e-01 4.1271874e-01]\n",
            " [5.1468146e-01 7.4787104e-01 5.9194779e-01 7.6678252e-01]\n",
            " [5.0643325e-01 5.0040275e-01 6.0071695e-01 5.2331966e-01]\n",
            " [0.0000000e+00 2.1112862e-01 6.5082580e-01 4.3438426e-01]\n",
            " [0.0000000e+00 7.0632058e-01 6.1716139e-01 8.6594021e-01]\n",
            " [4.8929816e-01 4.5427492e-01 5.7262021e-01 4.7639757e-01]\n",
            " [5.0920737e-01 4.1626489e-01 6.6901666e-01 4.5957711e-01]\n",
            " [4.6780142e-03 8.0310702e-01 1.5958229e-01 8.4036517e-01]\n",
            " [5.2617568e-01 5.6837583e-01 5.7943624e-01 5.8280307e-01]\n",
            " [5.0284761e-01 3.7398592e-01 6.4712596e-01 4.1297260e-01]\n",
            " [4.8591751e-01 4.4443721e-01 6.2469023e-01 4.7351986e-01]\n",
            " [5.7416862e-01 2.6725131e-01 6.5776157e-01 3.2031399e-01]\n",
            " [6.7198235e-01 9.4031775e-01 8.2117718e-01 9.8921400e-01]\n",
            " [5.2410471e-01 5.6155598e-01 5.7834709e-01 5.8050251e-01]\n",
            " [5.1758975e-01 7.5722069e-01 5.8831394e-01 7.7154583e-01]\n",
            " [5.2332848e-01 5.5781382e-01 5.7902896e-01 5.7355368e-01]\n",
            " [6.1236000e-01 4.2740157e-01 7.0609623e-01 4.8830026e-01]\n",
            " [0.0000000e+00 2.4423710e-01 6.0888764e-02 2.9377386e-01]\n",
            " [1.5484416e-02 1.9419364e-03 7.4516356e-01 2.5933647e-01]\n",
            " [4.9326646e-01 9.2395955e-01 8.3691311e-01 9.9770677e-01]\n",
            " [5.0529295e-01 3.6016637e-01 6.4336246e-01 3.9143851e-01]\n",
            " [8.4342342e-03 2.4212143e-01 4.9744964e-02 2.8314558e-01]\n",
            " [5.2210915e-01 5.3608811e-01 5.9767485e-01 5.5313313e-01]\n",
            " [5.1312602e-01 5.2381009e-01 6.0054040e-01 5.4296505e-01]\n",
            " [5.1831567e-01 5.0345343e-01 5.9754527e-01 5.2275288e-01]\n",
            " [5.2045572e-01 6.0093164e-01 6.4599109e-01 6.3436383e-01]\n",
            " [5.1316828e-01 6.7925382e-01 5.5048609e-01 6.9244283e-01]\n",
            " [4.2972320e-01 8.2874358e-01 5.9004873e-01 8.6437541e-01]\n",
            " [5.2659333e-01 6.2719083e-01 5.6328988e-01 6.5378499e-01]\n",
            " [5.0478107e-01 3.8941067e-01 6.1523116e-01 4.1995162e-01]\n",
            " [5.0132489e-01 3.6423624e-01 6.5975285e-01 4.0371999e-01]\n",
            " [5.7311028e-01 2.6673263e-01 6.6622359e-01 3.1864992e-01]\n",
            " [5.1510340e-01 6.2409180e-01 5.6383228e-01 6.5803182e-01]\n",
            " [8.3203152e-02 4.0756792e-01 5.8434421e-01 5.5831051e-01]\n",
            " [2.8820193e-01 4.6254270e-04 4.1427991e-01 3.6707692e-02]\n",
            " [6.2713271e-01 3.6099508e-01 7.0596063e-01 4.0978041e-01]\n",
            " [4.9715942e-01 4.5521107e-01 5.8427125e-01 4.7787207e-01]\n",
            " [1.1719427e-02 3.0807254e-01 9.7320050e-02 3.2507548e-01]\n",
            " [5.1589394e-01 3.8009039e-01 5.9697241e-01 4.1176715e-01]\n",
            " [5.1242912e-01 6.2364930e-01 5.6243664e-01 6.5768224e-01]\n",
            " [4.0077376e-01 8.8497424e-01 5.8165663e-01 9.3913031e-01]\n",
            " [0.0000000e+00 9.9475933e-03 1.3625400e-01 3.1597435e-02]\n",
            " [5.1390564e-01 5.2950239e-01 6.0205597e-01 5.5237609e-01]\n",
            " [5.1069164e-01 6.2403965e-01 5.6341004e-01 6.5817988e-01]\n",
            " [4.8037991e-01 6.2032777e-01 5.6528419e-01 6.6012335e-01]\n",
            " [5.3840739e-01 9.2802435e-01 7.1361738e-01 9.9945277e-01]\n",
            " [4.8633784e-01 6.2024730e-01 5.6352872e-01 6.6021770e-01]]\n"
          ]
        }
      ],
      "source": [
        "# runs the object detection model and prints information about the objects found\n",
        "run_detector(detector, downloaded_image_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C3_W2_Lab_1_Simple_Object_Detection.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('tf112')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf259153a16f0bc38a2ce5bca8735967bc36b047312e0a6f5ab67c35034c155f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
